{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f33d2ac",
   "metadata": {},
   "source": [
    "\n",
    "# 20 Newsgroups — End‑to‑End Experiments with MLflow\n",
    "\n",
    "This notebook contains two independent workflows on the 20 Newsgroups dataset:\n",
    "\n",
    "1. **PyCaret + MLflow** with TF‑IDF → SVD features, automated model comparison/tuning, and artifact logging.\n",
    "2. **PyTorch MLP + MLflow** with TF‑IDF → SVD features, manual training loop and logging.\n",
    "\n",
    "> **Notes (Windows users):**\n",
    "> - Keep artifact paths short if you run a local MLflow server to avoid long‑path issues.\n",
    "> - If PyCaret/MLflow versions in your environment are incompatible, pin `mlflow==2.12.1` and use a recent PyCaret (e.g., 3.3.x). \n",
    "> - If PyCaret raises a `sklearn` private API error, uncomment the small shim in the PyCaret section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25480369",
   "metadata": {},
   "source": [
    "## 1) PyCaret + MLflow (TF‑IDF → SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b13130f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ready: 20NG-PyCaret\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import mlflow\n",
    "\n",
    "EXPERIMENT_NAME = \"20NG-PyCaret\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\"); ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def close_all_runs():\n",
    "    while mlflow.active_run() is not None:\n",
    "        mlflow.end_run()\n",
    "\n",
    "close_all_runs()\n",
    "print(\"Experiment ready:\", EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "abd3b394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15076, 3770, 150)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = fetch_20newsgroups(subset='all')\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=30000, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_test_svd  = svd.transform(X_test_tfidf)\n",
    "\n",
    "joblib.dump(tfidf, ART_DIR / \"tfidf_20ng.joblib\")\n",
    "joblib.dump(svd,   ART_DIR / \"svd_20ng_150.joblib\")\n",
    "\n",
    "cols = [f\"svd_{i}\" for i in range(X_train_svd.shape[1])]\n",
    "train_df = pd.DataFrame(X_train_svd, columns=cols); train_df[\"label\"] = y_train\n",
    "test_df  = pd.DataFrame(X_test_svd,  columns=cols); test_df[\"label\"]  = y_test\n",
    "\n",
    "len(train_df), len(test_df), train_df.shape[1]-1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addec29b",
   "metadata": {},
   "source": [
    "## 2b. LLM Embeddings (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cd816ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LLM = False\n",
    "LLM_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "if USE_LLM:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer(LLM_MODEL)\n",
    "    X_train_emb = st_model.encode(X_train, batch_size=64, convert_to_numpy=True,\n",
    "                                  show_progress_bar=True, normalize_embeddings=True)\n",
    "    X_test_emb  = st_model.encode(X_test,  batch_size=64, convert_to_numpy=True,\n",
    "                                  show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "    X_train_svd = X_train_emb.astype(\"float32\")\n",
    "    X_test_svd  = X_test_emb.astype(\"float32\")\n",
    "\n",
    "    emb_cols = [f\"emb_{i}\" for i in range(X_train_svd.shape[1])]\n",
    "    train_df = pd.DataFrame(X_train_svd, columns=emb_cols); train_df[\"label\"] = y_train\n",
    "    test_df  = pd.DataFrame(X_test_svd,  columns=emb_cols);  test_df[\"label\"]  = y_test\n",
    "\n",
    "    FEATURE_SOURCE = f\"llm:{LLM_MODEL.split('/')[-1]}\"\n",
    "else:\n",
    "    FEATURE_SOURCE = \"tfidf+svd\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "754f6723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb', 'xgboost', 'rf', 'ridge', 'lightgbm', 'lr']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def _filter_available(models):\n",
    "    avail = set(models)\n",
    "    try:\n",
    "        import lightgbm  \n",
    "    except Exception:\n",
    "        avail.discard(\"lightgbm\")\n",
    "    try:\n",
    "        import xgboost  \n",
    "    except Exception:\n",
    "        avail.discard(\"xgboost\")\n",
    "    return list(avail)\n",
    "\n",
    "include_models = _filter_available([\"lr\", \"ridge\", \"nb\", \"rf\", \"lightgbm\", \"xgboost\"])\n",
    "include_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67ca9e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Description            Value\n",
      "0                    Session id               42\n",
      "1                        Target            label\n",
      "2                   Target type       Multiclass\n",
      "3           Original data shape     (15076, 151)\n",
      "4        Transformed data shape     (15076, 151)\n",
      "5   Transformed train set shape     (10553, 151)\n",
      "6    Transformed test set shape      (4523, 151)\n",
      "7              Numeric features              150\n",
      "8                    Preprocess             True\n",
      "9               Imputation type           simple\n",
      "10           Numeric imputation             mean\n",
      "11       Categorical imputation             mode\n",
      "12               Fold Generator  StratifiedKFold\n",
      "13                  Fold Number                3\n",
      "14                     CPU Jobs               -1\n",
      "15                      Use GPU            False\n",
      "16               Log Experiment     MlflowLogger\n",
      "17              Experiment Name     20NG-PyCaret\n",
      "18                          USI             773e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
      "lr             Logistic Regression    0.8199  0.0000  0.8199  0.8230  0.8156   \n",
      "ridge             Ridge Classifier    0.8184  0.0000  0.8184  0.8182  0.8119   \n",
      "xgboost  Extreme Gradient Boosting    0.8082  0.9842  0.8082  0.8087  0.8076   \n",
      "rf        Random Forest Classifier    0.7916  0.9760  0.7916  0.7951  0.7901   \n",
      "nb                     Naive Bayes    0.6751  0.9435  0.6751  0.7001  0.6802   \n",
      "\n",
      "          Kappa     MCC  TT (Sec)  \n",
      "lr       0.8102  0.8106    0.3267  \n",
      "ridge    0.8087  0.8091    0.9467  \n",
      "xgboost  0.7980  0.7981   13.3167  \n",
      "rf       0.7805  0.7807    3.9733  \n",
      "nb       0.6577  0.6587    1.8500  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8199</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>0.8156</td>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.8106</td>\n",
       "      <td>0.3267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.8182</td>\n",
       "      <td>0.8119</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.8091</td>\n",
       "      <td>0.9467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>Extreme Gradient Boosting</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>13.3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>0.7901</td>\n",
       "      <td>0.7805</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>3.9733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6751</td>\n",
       "      <td>0.9435</td>\n",
       "      <td>0.6751</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.6577</td>\n",
       "      <td>0.6587</td>\n",
       "      <td>1.8500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "lr             Logistic Regression    0.8199  0.0000  0.8199  0.8230  0.8156   \n",
       "ridge             Ridge Classifier    0.8184  0.0000  0.8184  0.8182  0.8119   \n",
       "xgboost  Extreme Gradient Boosting    0.8082  0.9842  0.8082  0.8087  0.8076   \n",
       "rf        Random Forest Classifier    0.7916  0.9760  0.7916  0.7951  0.7901   \n",
       "nb                     Naive Bayes    0.6751  0.9435  0.6751  0.7001  0.6802   \n",
       "\n",
       "          Kappa     MCC  TT (Sec)  \n",
       "lr       0.8102  0.8106    0.3267  \n",
       "ridge    0.8087  0.8091    0.9467  \n",
       "xgboost  0.7980  0.7981   13.3167  \n",
       "rf       0.7805  0.7807    3.9733  \n",
       "nb       0.6577  0.6587    1.8500  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model, finalize_model, predict_model, save_model, pull\n",
    ")\n",
    "\n",
    "clf = setup(\n",
    "    data=train_df,\n",
    "    target=\"label\",\n",
    "    session_id=42,\n",
    "    fold=3,\n",
    "    html=False,\n",
    "    log_experiment=True,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    experiment_custom_tags={\"dataset\":\"20newsgroups\",\"features\":\"tfidf+svd\"},\n",
    "    log_plots=True,\n",
    "    log_profile=False,\n",
    "    log_data=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best = compare_models(\n",
    "    include=include_models,\n",
    "    n_select=1,\n",
    "    turbo=False,\n",
    "    budget_time=300\n",
    ")\n",
    "\n",
    "leaderboard = pull()\n",
    "lb_path = ART_DIR / \"leaderboard.csv\"\n",
    "leaderboard.to_csv(lb_path, index=False)\n",
    "leaderboard.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c40e20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                       \n",
      "0       0.8368  0.0  0.8368  0.8399  0.8376  0.8281  0.8282\n",
      "1       0.8474  0.0  0.8474  0.8484  0.8476  0.8392  0.8393\n",
      "2       0.8388  0.0  0.8388  0.8438  0.8400  0.8302  0.8304\n",
      "Mean    0.8410  0.0  0.8410  0.8440  0.8417  0.8325  0.8326\n",
      "Std     0.0046  0.0  0.0046  0.0035  0.0043  0.0048  0.0048\n",
      "                 Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
      "0  Logistic Regression    0.8674  0.9919  0.8674  0.8681  0.8674  0.8603   \n",
      "\n",
      "      MCC  \n",
      "0  0.8604  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_0</th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_143</th>\n",
       "      <th>svd_144</th>\n",
       "      <th>svd_145</th>\n",
       "      <th>svd_146</th>\n",
       "      <th>svd_147</th>\n",
       "      <th>svd_148</th>\n",
       "      <th>svd_149</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051136</td>\n",
       "      <td>-0.044209</td>\n",
       "      <td>0.013876</td>\n",
       "      <td>-0.014207</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>-0.007518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.016458</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148950</td>\n",
       "      <td>-0.022029</td>\n",
       "      <td>-0.051318</td>\n",
       "      <td>-0.045028</td>\n",
       "      <td>0.060533</td>\n",
       "      <td>-0.028787</td>\n",
       "      <td>0.058711</td>\n",
       "      <td>0.029263</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>-0.011804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.037028</td>\n",
       "      <td>0.018487</td>\n",
       "      <td>-0.008069</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>-0.013960</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.127395</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>0.041384</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>-0.019644</td>\n",
       "      <td>-0.022073</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.022513</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>-0.015511</td>\n",
       "      <td>-0.039467</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.024015</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154591</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.014266</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>-0.034787</td>\n",
       "      <td>-0.003228</td>\n",
       "      <td>-0.012234</td>\n",
       "      <td>-0.059677</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027196</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>-0.060703</td>\n",
       "      <td>-0.016321</td>\n",
       "      <td>-0.039191</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155006</td>\n",
       "      <td>0.042561</td>\n",
       "      <td>-0.049434</td>\n",
       "      <td>-0.031397</td>\n",
       "      <td>0.037215</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>0.065599</td>\n",
       "      <td>0.019820</td>\n",
       "      <td>-0.005767</td>\n",
       "      <td>-0.027879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>0.023195</td>\n",
       "      <td>0.014267</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>-0.002615</td>\n",
       "      <td>-0.031948</td>\n",
       "      <td>-0.003028</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.7586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      svd_0     svd_1     svd_2     svd_3     svd_4     svd_5     svd_6  \\\n",
       "0  0.051136 -0.044209  0.013876 -0.014207  0.010195  0.018256  0.002139   \n",
       "1  0.148950 -0.022029 -0.051318 -0.045028  0.060533 -0.028787  0.058711   \n",
       "2  0.127395 -0.002190  0.041384 -0.001889 -0.019644 -0.022073 -0.007373   \n",
       "3  0.154591  0.003464  0.014266  0.020361 -0.034787 -0.003228 -0.012234   \n",
       "4  0.155006  0.042561 -0.049434 -0.031397  0.037215  0.020427  0.065599   \n",
       "\n",
       "      svd_7     svd_8     svd_9  ...   svd_143   svd_144   svd_145   svd_146  \\\n",
       "0  0.019463  0.004180 -0.007518  ...  0.013087  0.012224  0.010218  0.019890   \n",
       "1  0.029263 -0.000823 -0.011804  ...  0.003848  0.037028  0.018487 -0.008069   \n",
       "2  0.001285  0.006122 -0.001303  ...  0.018730  0.022513  0.012194 -0.015511   \n",
       "3 -0.059677  0.039645  0.025397  ... -0.027196  0.036850  0.000714  0.008930   \n",
       "4  0.019820 -0.005767 -0.027879  ...  0.004671  0.023195  0.014267  0.001620   \n",
       "\n",
       "    svd_147   svd_148   svd_149  label  prediction_label  prediction_score  \n",
       "0  0.007670  0.009534  0.016458      1                 1            0.6053  \n",
       "1 -0.015455 -0.013960  0.015685     19                19            0.2750  \n",
       "2 -0.039467  0.005832  0.024015      5                 5            0.4727  \n",
       "3 -0.060703 -0.016321 -0.039191      7                 7            0.3642  \n",
       "4 -0.002615 -0.031948 -0.003028     17                17            0.7586  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_tuned = tune_model(best, optimize=\"Accuracy\", n_iter=20, choose_better=True)\n",
    "final_model = finalize_model(best_tuned)\n",
    "\n",
    "test_preds = predict_model(final_model, data=test_df)\n",
    "pred_path = ART_DIR / \"test_predictions_head.csv\"\n",
    "test_preds.head(50).to_csv(pred_path, index=False)\n",
    "test_preds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d868a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "PyCaret section completed. Check MLflow experiment: 20NG-PyCaret\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_model(final_model, str(ART_DIR / \"best_20ng_pycaret\"))\n",
    "\n",
    "close_all_runs() \n",
    "\n",
    "with mlflow.start_run(run_name=\"extra_artifacts_attach\"):\n",
    "    mlflow.log_params({\n",
    "        \"vectorizer\":\"tfidf\",\n",
    "        \"tfidf_stop_words\":\"english\",\n",
    "        \"tfidf_max_features\":30000,\n",
    "        \"tfidf_sublinear_tf\":True,\n",
    "        \"svd_n_components\":150,\n",
    "        \"test_size\":0.2,\n",
    "        \"random_state\":42\n",
    "    })\n",
    "    mlflow.log_artifact(str(ART_DIR / \"tfidf_20ng.joblib\"), artifact_path=\"preprocessing\")\n",
    "    mlflow.log_artifact(str(ART_DIR / \"svd_20ng_150.joblib\"), artifact_path=\"preprocessing\")\n",
    "    mlflow.log_artifact(str(ART_DIR / \"leaderboard.csv\"), artifact_path=\"reports\")\n",
    "    mlflow.log_artifact(str(ART_DIR / \"test_predictions_head.csv\"), artifact_path=\"reports\")\n",
    "    mlflow.log_artifact(str(ART_DIR / \"best_20ng_pycaret.pkl\"), artifact_path=\"model_pickles\")\n",
    "\n",
    "print(\"PyCaret section completed. Check MLflow experiment:\", EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6910d",
   "metadata": {},
   "source": [
    "## 2) PyTorch MLP + MLflow (TF‑IDF → SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b20f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | test acc=0.6618\n",
      "Epoch 02 | test acc=0.7857\n",
      "Epoch 03 | test acc=0.8284\n",
      "Epoch 04 | test acc=0.8432\n",
      "Epoch 05 | test acc=0.8538\n",
      "Epoch 06 | test acc=0.8578\n",
      "Epoch 07 | test acc=0.8626\n",
      "Epoch 08 | test acc=0.8650\n",
      "Epoch 09 | test acc=0.8623\n",
      "Epoch 10 | test acc=0.8668\n",
      "Epoch 11 | test acc=0.8698\n",
      "Epoch 12 | test acc=0.8690\n",
      "Epoch 13 | test acc=0.8714\n",
      "Epoch 14 | test acc=0.8711\n",
      "Epoch 15 | test acc=0.8751\n",
      "Epoch 16 | test acc=0.8724\n",
      "Epoch 17 | test acc=0.8753\n",
      "Epoch 18 | test acc=0.8772\n",
      "Epoch 19 | test acc=0.8729\n",
      "Epoch 20 | test acc=0.8780\n",
      "Epoch 21 | test acc=0.8788\n",
      "Epoch 22 | test acc=0.8759\n",
      "Epoch 23 | test acc=0.8790\n",
      "Epoch 24 | test acc=0.8796\n",
      "Epoch 25 | test acc=0.8809\n",
      "Epoch 26 | test acc=0.8772\n",
      "Epoch 27 | test acc=0.8817\n",
      "Epoch 28 | test acc=0.8825\n",
      "Epoch 29 | test acc=0.8828\n",
      "Epoch 30 | test acc=0.8849\n",
      "Epoch 31 | test acc=0.8849\n",
      "Epoch 32 | test acc=0.8812\n",
      "Epoch 33 | test acc=0.8854\n",
      "Epoch 34 | test acc=0.8838\n",
      "Epoch 35 | test acc=0.8828\n",
      "Epoch 36 | test acc=0.8838\n",
      "Epoch 37 | test acc=0.8854\n",
      "Epoch 38 | test acc=0.8891\n",
      "Epoch 39 | test acc=0.8894\n",
      "Epoch 40 | test acc=0.8854\n",
      "Epoch 41 | test acc=0.8897\n",
      "Epoch 42 | test acc=0.8907\n",
      "Epoch 43 | test acc=0.8899\n",
      "Epoch 44 | test acc=0.8897\n",
      "Epoch 45 | test acc=0.8899\n",
      "Epoch 46 | test acc=0.8915\n",
      "Epoch 47 | test acc=0.8870\n",
      "Epoch 48 | test acc=0.8926\n",
      "Epoch 49 | test acc=0.8889\n",
      "Epoch 50 | test acc=0.8886\n",
      "MLP test accuracy: 0.8926\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.883     0.900     0.892       160\n",
      "           comp.graphics      0.805     0.826     0.815       195\n",
      " comp.os.ms-windows.misc      0.868     0.802     0.834       197\n",
      "comp.sys.ibm.pc.hardware      0.757     0.781     0.769       196\n",
      "   comp.sys.mac.hardware      0.844     0.839     0.842       193\n",
      "          comp.windows.x      0.900     0.909     0.905       198\n",
      "            misc.forsale      0.867     0.867     0.867       195\n",
      "               rec.autos      0.891     0.904     0.897       198\n",
      "         rec.motorcycles      0.964     0.930     0.946       199\n",
      "      rec.sport.baseball      0.955     0.960     0.957       199\n",
      "        rec.sport.hockey      0.980     0.970     0.975       200\n",
      "               sci.crypt      0.969     0.955     0.962       198\n",
      "         sci.electronics      0.823     0.848     0.835       197\n",
      "                 sci.med      0.920     0.924     0.922       198\n",
      "               sci.space      0.907     0.944     0.925       197\n",
      "  soc.religion.christian      0.926     0.940     0.933       199\n",
      "      talk.politics.guns      0.874     0.912     0.892       182\n",
      "   talk.politics.mideast      0.994     0.936     0.964       188\n",
      "      talk.politics.misc      0.907     0.877     0.892       155\n",
      "      talk.religion.misc      0.798     0.786     0.792       126\n",
      "\n",
      "                accuracy                          0.893      3770\n",
      "               macro avg      0.892     0.890     0.891      3770\n",
      "            weighted avg      0.893     0.893     0.893      3770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/31 20:07:46 WARNING mlflow.utils.requirements_utils: Found torch version (2.9.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/10/31 20:07:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.9.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['comp.sys.mac.hardware', 'alt.atheism']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# ============== TOGGLE ==============\n",
    "USE_EMB = False\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMB_BATCH = 128\n",
    "MAX_SEQ_LEN = 256   # trims long emails → faster with minimal loss\n",
    "mlflow.set_experiment(\"20ng-mlp-emb\" if USE_EMB else \"20ng-mlp-svd\")\n",
    "# ====================================\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all')\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Feature extraction: TF-IDF+SVD or SentenceTransformer embeddings ---\n",
    "st = None\n",
    "if not USE_EMB:\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=30000, sublinear_tf=True)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "    X_train_feats = svd.fit_transform(X_train_tfidf).astype(\"float32\")\n",
    "    X_test_feats  = svd.transform(X_test_tfidf).astype(\"float32\")\n",
    "    FEATURE_SRC = \"tfidf+svd\"\n",
    "else:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    st = SentenceTransformer(EMB_MODEL, device=str(device))\n",
    "    st.max_seq_length = MAX_SEQ_LEN\n",
    "\n",
    "    X_train_feats = st.encode(\n",
    "        X_train, batch_size=EMB_BATCH, convert_to_numpy=True,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "    X_test_feats = st.encode(\n",
    "        X_test, batch_size=EMB_BATCH, convert_to_numpy=True,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "    FEATURE_SRC = f\"emb:{EMB_MODEL.split('/')[-1]}\"\n",
    "\n",
    "# --- Torch setup / loaders ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(np.unique(y_train))\n",
    "in_dim = X_train_feats.shape[1]\n",
    "\n",
    "Xtr = torch.tensor(X_train_feats, dtype=torch.float32)\n",
    "ytr = torch.tensor(y_train, dtype=torch.long)\n",
    "Xte = torch.tensor(X_test_feats,  dtype=torch.float32)\n",
    "yte = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(Xte, yte), batch_size=512, shuffle=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),    nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP(in_dim, num_classes).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_acc, patience, wait = 0.0, 5, 0\n",
    "\n",
    "# -------- Word-importance helpers (NN on SVD). Run only if not USE_EMB --------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def nn_class_word_importance_svd(model, svd, tfidf, X_feats, y_true, class_idx,\n",
    "                                 top_k=20, device=torch.device(\"cpu\"),\n",
    "                                 use_grad_times_input=True, only_correct=True):\n",
    "    model.eval()\n",
    "    feature_dim = X_feats.shape[1]\n",
    "    vocab = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_feats.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    mask = (y_true == class_idx)\n",
    "    if only_correct:\n",
    "        mask = mask & (preds == class_idx)\n",
    "\n",
    "    Xc = X_feats[mask]\n",
    "    if Xc.shape[0] == 0:\n",
    "        raise ValueError(\"No (correct) samples for this class; try only_correct=False.\")\n",
    "\n",
    "    sal_sum = torch.zeros(feature_dim, dtype=torch.float32, device=device)\n",
    "    bs = 512\n",
    "    for i in range(0, Xc.shape[0], bs):\n",
    "        xb = Xc[i:i+bs].to(device).clone().detach()\n",
    "        xb.requires_grad_(True)\n",
    "        out = model(xb)[:, class_idx].sum()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out.backward()\n",
    "        grad = xb.grad.detach()\n",
    "        sal = (grad * xb).mean(dim=0) if use_grad_times_input else grad.abs().mean(dim=0)\n",
    "        sal_sum += sal\n",
    "\n",
    "    sal_svd = (sal_sum / (Xc.shape[0] / min(Xc.shape[0], bs))).detach().cpu().numpy()\n",
    "    word_scores = svd.components_.T @ sal_svd\n",
    "\n",
    "    pos_idx = np.argsort(word_scores)[-top_k:][::-1]\n",
    "    neg_idx = np.argsort(word_scores)[:top_k]\n",
    "    df = pd.DataFrame({\n",
    "        \"Word\": np.concatenate([vocab[neg_idx], vocab[pos_idx]]),\n",
    "        \"Weight\": np.concatenate([word_scores[neg_idx], word_scores[pos_idx]]),\n",
    "        \"Effect\": [\"negative\"]*top_k + [\"positive\"]*top_k\n",
    "    })\n",
    "    return df.sort_values(by=[\"Effect\",\"Weight\"], ascending=[True, True])\n",
    "\n",
    "def plot_class_importance(df, target_class, out_path=None):\n",
    "    from matplotlib.patches import Patch\n",
    "    plt.figure(figsize=(11,6))\n",
    "    colors = df[\"Effect\"].map({\"positive\":\"tab:blue\", \"negative\":\"tab:red\"})\n",
    "    plt.barh(df[\"Word\"], df[\"Weight\"], color=colors)\n",
    "    plt.xlabel(\"Projected weight (word-space)\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.title(f\"Word importance for class: {target_class}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend(handles=[Patch(color=\"tab:red\", label=\"negative\"),\n",
    "                        Patch(color=\"tab:blue\", label=\"positive\")],\n",
    "               title=\"Effect\", loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "with mlflow.start_run(run_name=\"mlp-emb-baseline\" if USE_EMB else \"mlp-svd-baseline\"):\n",
    "    params = {\n",
    "        \"feature_source\": (\"emb:\"+EMB_MODEL.split(\"/\")[-1]) if USE_EMB else \"tfidf+svd\",\n",
    "        \"model_hidden_1\": 512, \"model_hidden_2\": 256, \"dropout\": 0.3,\n",
    "        \"optimizer\": \"Adam\", \"lr\": 1e-3, \"weight_decay\": 1e-4,\n",
    "        \"batch_size\": 256, \"patience\": patience, \"device\": str(device),\n",
    "        \"num_classes\": int(num_classes), \"input_dim\": int(in_dim),\n",
    "        \"random_state\": 42, \"test_size\": 0.2,\n",
    "    }\n",
    "    if USE_EMB:\n",
    "        params.update({\"emb_model\": EMB_MODEL, \"emb_batch\": EMB_BATCH,\n",
    "                       \"emb_max_seq_len\": MAX_SEQ_LEN, \"emb_normalized\": True})\n",
    "    else:\n",
    "        params.update({\"tfidf_stop_words\": \"english\", \"tfidf_max_features\": 30000,\n",
    "                       \"tfidf_sublinear_tf\": True, \"svd_n_components\": 200})\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # ---- Train\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(Xte.to(device))\n",
    "            preds = logits.argmax(dim=1)\n",
    "            acc = (preds.cpu() == yte).float().mean().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | test acc={acc:.4f}\")\n",
    "        mlflow.log_metric(\"test_accuracy\", acc, step=epoch+1)\n",
    "\n",
    "        if acc > best_acc + 1e-4:\n",
    "            best_acc, wait = acc, 0\n",
    "            torch.save(model.state_dict(), \"mlp_best.pt\")\n",
    "            mlflow.log_metric(\"best_accuracy\", best_acc, step=epoch+1)\n",
    "            mlflow.log_artifact(\"mlp_best.pt\", artifact_path=\"checkpoints\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stop\"); break\n",
    "\n",
    "    # ---- Load best & evaluate\n",
    "    try:\n",
    "        state = torch.load(\"mlp_best.pt\", map_location=device, weights_only=True)\n",
    "    except TypeError:\n",
    "        state = torch.load(\"mlp_best.pt\", map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(Xte.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    final_acc = accuracy_score(y_test, preds)\n",
    "    print(f\"MLP test accuracy: {final_acc:.4f}\")\n",
    "    mlflow.log_metric(\"final_test_accuracy\", final_acc)\n",
    "\n",
    "    report_str = classification_report(y_test, preds, target_names=dataset.target_names, digits=3)\n",
    "    print(report_str)\n",
    "    Path(\"reports\").mkdir(exist_ok=True)\n",
    "    with open(\"reports/classification_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_str)\n",
    "    mlflow.log_artifact(\"reports/classification_report.txt\", artifact_path=\"reports\")\n",
    "\n",
    "    # ---- Persist preprocessing artifacts\n",
    "    if not USE_EMB:\n",
    "        joblib.dump(tfidf, \"tfidf_20ng.joblib\")\n",
    "        joblib.dump(svd,   \"svd_20ng_200.joblib\")\n",
    "        mlflow.log_artifact(\"tfidf_20ng.joblib\",   artifact_path=\"preprocessing\")\n",
    "        mlflow.log_artifact(\"svd_20ng_200.joblib\", artifact_path=\"preprocessing\")\n",
    "    else:\n",
    "        with open(\"embedder.txt\", \"w\") as f:\n",
    "            f.write(f\"{EMB_MODEL}\\nmax_seq_len={MAX_SEQ_LEN}\\nnormalize=True\\nbatch={EMB_BATCH}\\n\")\n",
    "        mlflow.log_artifact(\"embedder.txt\", artifact_path=\"preprocessing\")\n",
    "\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\", registered_model_name=None)\n",
    "\n",
    "    # ---- Class-wise word importance (only for TF-IDF+SVD)\n",
    "    if not USE_EMB:\n",
    "        out_dir = Path(\"word_importance\"); out_dir.mkdir(exist_ok=True)\n",
    "        for cls_name in dataset.target_names:\n",
    "            cls_idx = dataset.target_names.index(cls_name)\n",
    "            df_imp = nn_class_word_importance_svd(\n",
    "                model=model, svd=svd, tfidf=tfidf,\n",
    "                X_feats=Xte, y_true=y_test, class_idx=cls_idx,\n",
    "                top_k=20, device=device, use_grad_times_input=True, only_correct=True\n",
    "            )\n",
    "            slug = cls_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "            csv_path = out_dir / f\"word_importance_{cls_idx:02d}_{slug}.csv\"\n",
    "            png_path = out_dir / f\"word_importance_{cls_idx:02d}_{slug}.png\"\n",
    "            df_imp.to_csv(csv_path, index=False)\n",
    "            plot_class_importance(df_imp, cls_name, out_path=str(png_path))\n",
    "            mlflow.log_artifact(str(csv_path), artifact_path=\"word_importance\")\n",
    "            mlflow.log_artifact(str(png_path), artifact_path=\"word_importance\")\n",
    "\n",
    "# --- Inference helper (uses the same pipeline as training) ---\n",
    "def predict_texts(texts):\n",
    "    model.eval()\n",
    "    if not USE_EMB:\n",
    "        X_ = svd.transform(tfidf.transform(texts))\n",
    "        Xt = torch.tensor(X_, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        X_ = st.encode(texts, batch_size=EMB_BATCH, convert_to_numpy=True,\n",
    "                       show_progress_bar=False, normalize_embeddings=True).astype(\"float32\")\n",
    "        Xt = torch.tensor(X_, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(Xt)\n",
    "        labels = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "    return [dataset.target_names[i] for i in labels]\n",
    "\n",
    "# quick smoke test\n",
    "predict_texts([\n",
    "    \"GPU driver fails on my Mac laptop\",\n",
    "    \"Theology debate about atheism and religion\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e010cb",
   "metadata": {},
   "source": [
    "Metrics for the best neural network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104aaf6e",
   "metadata": {},
   "source": [
    "![image.png](screenshots/nn_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbdbd7",
   "metadata": {},
   "source": [
    "![Alt text](screenshots/cr_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636f338",
   "metadata": {},
   "source": [
    "The best model chosen by PyCaret is Logistic Regression. The metrics are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1552a",
   "metadata": {},
   "source": [
    "![image.png](screenshots/lr_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4fa00",
   "metadata": {},
   "source": [
    "![image.png](screenshots/lr_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de78ea",
   "metadata": {},
   "source": [
    "Features importances:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1334b",
   "metadata": {},
   "source": [
    "![image.png](screenshots/features_importances_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c62dd",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e39453",
   "metadata": {},
   "source": [
    "![Alt text](screenshots/lr_cm.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
